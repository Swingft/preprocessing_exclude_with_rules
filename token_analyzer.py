#!/usr/bin/env python3
"""
JSONL í† í° ë¶„ì„ê¸°

JSONL íŒŒì¼ì˜ ê° ì¤„ë³„ í† í° ìˆ˜ë¥¼ ê³„ì‚°í•˜ê³  í†µê³„ ì œê³µ
"""

import json
import tiktoken
from pathlib import Path
from typing import Dict, List, Tuple
from dataclasses import dataclass


@dataclass
class TokenStats:
    """í† í° í†µê³„"""
    line_num: int
    total_tokens: int
    instruction_tokens: int
    input_tokens: int
    output_tokens: int
    content: Dict


def count_tokens(text: str, encoding_name: str = "cl100k_base") -> int:
    """
    í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ ê³„ì‚°

    Args:
        text: í…ìŠ¤íŠ¸
        encoding_name: tiktoken ì¸ì½”ë”© (GPT-4/3.5: cl100k_base)

    Returns:
        í† í° ìˆ˜
    """
    encoding = tiktoken.get_encoding(encoding_name)
    return len(encoding.encode(text))


def analyze_jsonl_line(line: str, line_num: int) -> TokenStats:
    """
    JSONL í•œ ì¤„ ë¶„ì„

    Args:
        line: JSONL í•œ ì¤„
        line_num: ì¤„ ë²ˆí˜¸

    Returns:
        í† í° í†µê³„
    """
    data = json.loads(line)

    instruction = data.get("instruction", "")
    input_text = data.get("input", "")
    output = data.get("output", "")

    instruction_tokens = count_tokens(instruction)
    input_tokens = count_tokens(input_text)
    output_tokens = count_tokens(output)
    total_tokens = instruction_tokens + input_tokens + output_tokens

    return TokenStats(
        line_num=line_num,
        total_tokens=total_tokens,
        instruction_tokens=instruction_tokens,
        input_tokens=input_tokens,
        output_tokens=output_tokens,
        content=data
    )


def analyze_jsonl_file(file_path: Path) -> List[TokenStats]:
    """
    JSONL íŒŒì¼ ì „ì²´ ë¶„ì„

    Args:
        file_path: JSONL íŒŒì¼ ê²½ë¡œ

    Returns:
        ê° ì¤„ì˜ í† í° í†µê³„ ë¦¬ìŠ¤íŠ¸
    """
    stats_list = []

    with open(file_path, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            line = line.strip()
            if not line:
                continue

            try:
                stats = analyze_jsonl_line(line, line_num)
                stats_list.append(stats)
            except Exception as e:
                print(f"âš ï¸  Line {line_num} ì—ëŸ¬: {e}")

    return stats_list


def print_statistics(stats_list: List[TokenStats]):
    """
    í† í° í†µê³„ ì¶œë ¥

    Args:
        stats_list: í† í° í†µê³„ ë¦¬ìŠ¤íŠ¸
    """
    if not stats_list:
        print("âŒ ë°ì´í„° ì—†ìŒ")
        return

    # ì „ì²´ í†µê³„
    total_lines = len(stats_list)
    total_tokens_all = sum(s.total_tokens for s in stats_list)
    avg_tokens = total_tokens_all / total_lines

    max_stat = max(stats_list, key=lambda s: s.total_tokens)
    min_stat = min(stats_list, key=lambda s: s.total_tokens)

    # í—¤ë”
    print("\n" + "=" * 80)
    print("ğŸ“Š JSONL í† í° ë¶„ì„ ê²°ê³¼")
    print("=" * 80)

    # ì „ì²´ í†µê³„
    print(f"\nğŸ“ˆ ì „ì²´ í†µê³„:")
    print(f"  â€¢ ì´ ë¼ì¸ ìˆ˜: {total_lines:,}")
    print(f"  â€¢ ì´ í† í° ìˆ˜: {total_tokens_all:,}")
    print(f"  â€¢ í‰ê·  í† í°: {avg_tokens:,.1f}")
    print(f"  â€¢ ìµœëŒ€ í† í°: {max_stat.total_tokens:,} (Line {max_stat.line_num})")
    print(f"  â€¢ ìµœì†Œ í† í°: {min_stat.total_tokens:,} (Line {min_stat.line_num})")

    # í•„ë“œë³„ í‰ê· 
    avg_instruction = sum(s.instruction_tokens for s in stats_list) / total_lines
    avg_input = sum(s.input_tokens for s in stats_list) / total_lines
    avg_output = sum(s.output_tokens for s in stats_list) / total_lines

    print(f"\nğŸ“‹ í•„ë“œë³„ í‰ê· :")
    print(f"  â€¢ instruction: {avg_instruction:,.1f} í† í°")
    print(f"  â€¢ input:       {avg_input:,.1f} í† í°")
    print(f"  â€¢ output:      {avg_output:,.1f} í† í°")

    # í† í° ë²”ìœ„ ë¶„í¬
    print(f"\nğŸ“Š í† í° ë²”ìœ„ ë¶„í¬:")
    ranges = [
        (0, 1000, "0-1K"),
        (1000, 2000, "1K-2K"),
        (2000, 5000, "2K-5K"),
        (5000, 10000, "5K-10K"),
        (10000, 20000, "10K-20K"),
        (20000, float('inf'), "20K+")
    ]

    for min_tok, max_tok, label in ranges:
        count = sum(1 for s in stats_list if min_tok <= s.total_tokens < max_tok)
        if count > 0:
            pct = count / total_lines * 100
            bar = "â–ˆ" * int(pct / 2)
            print(f"  {label:>8}: {count:3} ({pct:5.1f}%) {bar}")

    # ì •ë ¬ëœ ìƒì„¸ ëª©ë¡
    print(f"\nğŸ“ ë¼ì¸ë³„ ìƒì„¸ (í† í° ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ):")
    print("=" * 80)
    print(f"{'Line':>6} â”‚ {'Total':>8} â”‚ {'Instruction':>12} â”‚ {'Input':>8} â”‚ {'Output':>8}")
    print("â”€" * 80)

    sorted_stats = sorted(stats_list, key=lambda s: s.total_tokens, reverse=True)

    for stats in sorted_stats:
        print(f"{stats.line_num:6} â”‚ {stats.total_tokens:8,} â”‚ "
              f"{stats.instruction_tokens:12,} â”‚ {stats.input_tokens:8,} â”‚ "
              f"{stats.output_tokens:8,}")

    print("=" * 80)

    # ê¶Œì¥ ì„¤ì •
    print(f"\nğŸ’¡ ê¶Œì¥ ì„¤ì •:")

    # 99 percentile
    sorted_totals = sorted([s.total_tokens for s in stats_list])
    p99_idx = int(len(sorted_totals) * 0.99)
    p99_tokens = sorted_totals[p99_idx] if p99_idx < len(sorted_totals) else sorted_totals[-1]

    print(f"  â€¢ max_tokens (99%): {p99_tokens:,}")
    print(f"  â€¢ max_tokens (ìµœëŒ€): {max_stat.total_tokens:,}")

    # íŒ¨ë”© ê³ ë ¤
    recommended_max = ((max_stat.total_tokens + 511) // 512) * 512  # 512 ë‹¨ìœ„ë¡œ ì˜¬ë¦¼
    print(f"  â€¢ ê¶Œì¥ max_tokens: {recommended_max:,} (512 ë‹¨ìœ„)")

    # Context window ê²½ê³ 
    if max_stat.total_tokens > 4096:
        print(f"\nâš ï¸  ê²½ê³ : ìµœëŒ€ í† í°ì´ 4Kë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤. GPT-3.5ëŠ” ë¶ˆê°€ëŠ¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    if max_stat.total_tokens > 8192:
        print(f"âš ï¸  ê²½ê³ : ìµœëŒ€ í† í°ì´ 8Kë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤. ì¼ë¶€ ëª¨ë¸ì—ì„œ ë¬¸ì œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    if max_stat.total_tokens > 16384:
        print(f"âš ï¸  ê²½ê³ : ìµœëŒ€ í† í°ì´ 16Kë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤. ê¸´ ì»¨í…ìŠ¤íŠ¸ ëª¨ë¸ í•„ìš”í•©ë‹ˆë‹¤.")


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    import argparse

    parser = argparse.ArgumentParser(
        description="JSONL íŒŒì¼ì˜ í† í° ìˆ˜ ë¶„ì„"
    )
    parser.add_argument(
        'file',
        type=str,
        help='JSONL íŒŒì¼ ê²½ë¡œ'
    )
    parser.add_argument(
        '--encoding',
        type=str,
        default='cl100k_base',
        help='tiktoken ì¸ì½”ë”© (ê¸°ë³¸: cl100k_base for GPT-4/3.5-turbo)'
    )
    parser.add_argument(
        '--sort-by',
        type=str,
        choices=['total', 'instruction', 'input', 'output'],
        default='total',
        help='ì •ë ¬ ê¸°ì¤€'
    )

    args = parser.parse_args()

    file_path = Path(args.file)

    if not file_path.exists():
        print(f"âŒ íŒŒì¼ ì—†ìŒ: {file_path}")
        return 1

    print(f"ğŸ” ë¶„ì„ ì¤‘: {file_path}")
    print(f"ğŸ“ ì¸ì½”ë”©: {args.encoding}")

    # ë¶„ì„
    stats_list = analyze_jsonl_file(file_path)

    # í†µê³„ ì¶œë ¥
    print_statistics(stats_list)

    return 0


if __name__ == "__main__":
    exit(main())